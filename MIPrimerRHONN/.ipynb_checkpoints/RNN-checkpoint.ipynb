{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "This is a pure numpy implementation of word generation using an RNN\n",
    "\n",
    "![alt text](http://corochann.com/wp-content/uploads/2017/05/text_sequence_predict.png \"Logo Title Text 1\")\n",
    "\n",
    "We're going to have our network learn how to predict the next words in a given paragraph. This will require a recurrent architecture since the network will have to remember a sequence of characters. The order matters. 1000 iterations and we'll have pronouncable english. The longer the training time the better. You can feed it any text sequence (words, python, HTML, etc.)\n",
    "\n",
    "## What is a Recurrent Network?\n",
    "\n",
    "Feedforward networks are great for learning a pattern between a set of inputs and outputs.\n",
    "![alt text](https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Fig-1-Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://s-media-cache-ak0.pinimg.com/236x/10/29/a9/1029a9a0534a768b4c4c2b5341bdd003--city-year-math-patterns.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Hamza_Guellue/publication/223079746/figure/fig5/AS:305255788105731@1449790059371/Fig-5-Configuration-of-a-three-layered-feed-forward-neural-network.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "- temperature & location\n",
    "- height & weight\n",
    "- car speed and brand\n",
    "\n",
    "But what if the ordering of the data matters? \n",
    "\n",
    "![alt text](http://www.aboutcurrency.com/images/university/fxvideocourse/google_chart.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2016/vondrick-machine-learning-behavior-algorithm-mit-csail_0.jpg?itok=ruGmLJm2 \"Logo Title Text 1\")\n",
    "\n",
    "Alphabet, Lyrics of a song. These are stored using Conditional Memory. You can only access an element if you have access to the previous elements (like a linkedlist). \n",
    "\n",
    "Enter recurrent networks\n",
    "\n",
    "We feed the hidden state from the previous time step back into the the network at the next time step.\n",
    "\n",
    "![alt text](https://iamtrask.github.io/img/basic_recurrence_singleton.png \"Logo Title Text 1\")\n",
    "\n",
    "So instead of the data flow operation happening like this\n",
    "\n",
    "## input -> hidden -> output\n",
    "\n",
    "it happens like this\n",
    "\n",
    "## (input + prev_hidden) -> hidden -> output\n",
    "\n",
    "wait. Why not this?\n",
    "\n",
    "## (input + prev_input) -> hidden -> output\n",
    "\n",
    "Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ferret-rnn-151211092908/95/recurrent-neural-networks-part-1-theory-10-638.jpg?cb=1449826311 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.mathworks.com/help/examples/nnet/win64/RefLayRecNetExample_01.png \"Logo Title Text 1\")\n",
    "\n",
    "RNN Formula\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*TUFnE2arCrMrCvxH.png \"Logo Title Text 1\")\n",
    "\n",
    "It basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n",
    "\n",
    "Loss function\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*ZsEG2aWfgqtk9Qk5. \"Logo Title Text 1\")\n",
    "\n",
    "The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood\n",
    "of y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence \n",
    "\n",
    "\n",
    "## Our steps\n",
    "\n",
    "- Initialize weights randomly\n",
    "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
    "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
    "- Measure error (the distance between the previous probability and the target char)\n",
    "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
    "- update all parameters in the direction via gradients that help to minimise the loss\n",
    "- Repeat! Until our loss is small AF\n",
    "\n",
    "## What are some use cases?\n",
    "\n",
    "- Time series prediction (weather forecasting, stock prices, traffic volume, etc. )\n",
    "- Sequential data generation (music, video, audio, etc.)\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)\n",
    "\n",
    "## What's next? \n",
    "\n",
    "1 LSTM Networks\n",
    "2 Bidirectional networks\n",
    "3 recursive networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code contains 4 parts\n",
    "* Load the trainning data\n",
    "  * encode char into vectors\n",
    "* Define the Recurrent Network\n",
    "* Define a loss function\n",
    "  * Forward pass\n",
    "  * Loss\n",
    "  * Backward pass\n",
    "* Define a function to create sentences from the model\n",
    "* Train the network\n",
    "  * Feed the network\n",
    "  * Calculate gradient and update the model parameters\n",
    "  * Output a text to see the progress of the training\n",
    " \n",
    "\n",
    "## Load the training data\n",
    "\n",
    "The network need a big txt file as an input.\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "I use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 118560 chars, 62 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "\n",
    "Neural networks operate on vectors (a vector is an array of float)\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "We'll count the number of unique chars (*vocab_size*). That will be the size of the vector. \n",
    "The vector contains only zero exept for the position of the char wherae the value is 1.\n",
    "\n",
    "#### So First let's calculate the *vocab_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 0, 'b': 1, 'g': 3, ')': 42, 'M': 4, 'n': 5, 'N': 6, 'k': 7, 'w': 8, 'x': 10, 'i': 12, 'Y': 2, '(': 13, 'j': 14, 'G': 15, 'a': 16, 't': 17, 'F': 36, 'u': 18, 'm': 19, '\\n': 20, 'y': 26, 'c': 21, '-': 49, 'B': 23, 'p': 56, 'z': 24, 'S': 25, 'A': 27, 'U': 30, ' ': 29, 'O': 31, 'T': 32, ';': 33, ',': 34, 'J': 11, 'ç': 59, 'W': 38, ':': 39, 'q': 40, 'd': 41, 'C': 43, 'v': 9, 'o': 44, 's': 28, '\"': 45, 'I': 22, 'D': 35, 'e': 47, 'l': 48, 'P': 37, '!': 51, \"'\": 50, 'V': 52, 'E': 53, '.': 54, 'H': 55, 'L': 57, 'h': 58, '?': 60, 'r': 61, 'Q': 46}\n",
      "{0: 'f', 1: 'b', 2: 'Y', 3: 'g', 4: 'M', 5: 'n', 6: 'N', 7: 'k', 8: 'w', 9: 'v', 10: 'x', 11: 'J', 12: 'i', 13: '(', 14: 'j', 15: 'G', 16: 'a', 17: 't', 18: 'u', 19: 'm', 20: '\\n', 21: 'c', 22: 'I', 23: 'B', 24: 'z', 25: 'S', 26: 'y', 27: 'A', 28: 's', 29: ' ', 30: 'U', 31: 'O', 32: 'T', 33: ';', 34: ',', 35: 'D', 36: 'F', 37: 'P', 38: 'W', 39: ':', 40: 'q', 41: 'd', 42: ')', 43: 'C', 44: 'o', 45: '\"', 46: 'Q', 47: 'e', 48: 'l', 49: '-', 50: \"'\", 51: '!', 52: 'V', 53: 'E', 54: '.', 55: 'H', 56: 'p', 57: 'L', 58: 'h', 59: 'ç', 60: '?', 61: 'r'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we create 2 dictionary to encode and decode a char to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 'a')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix['a'],ix_to_char[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finaly we create a vector from a char like this:\n",
    "The dictionary defined above allosw us to create a vector of size 61 instead of 256.  \n",
    "Here and exemple of the char 'a'  \n",
    "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "\n",
    "The neural network is made of 3 layers:\n",
    "* an input layer\n",
    "* an hidden layer\n",
    "* an output layer\n",
    "\n",
    "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias\n",
    "\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "The __loss__ is a key concept in all neural networks training. \n",
    "It is a value that describe how good is our model.  \n",
    "The smaller the loss, the better our model is.  \n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "  \n",
    "During the training phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculates the loss but also the gradients (see backward pass):\n",
    "* It perform a forward pass: calculate the next char given a char from the training set.\n",
    "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "* It calculate the backward pass to calculate the gradients \n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function outputs:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
    "\n",
    "xs[t] is the vector that encode the char at position t\n",
    "ps[t] is the probabilities for next char\n",
    "\n",
    "![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "```\n",
    "\n",
    "or is dirty pseudo code for each char\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
    "ys = hs*Why + by\n",
    "ps = normalized(ys)\n",
    "```\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
    "This is possible but would be time consuming.\n",
    "There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n",
    "Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n",
    "\n",
    "#### goal is to calculate gradients for the forward formula:\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
    "ys = hs*Why + by\n",
    "```\n",
    "\n",
    "The loss for one datapoint\n",
    "![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n",
    "\n",
    "How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n",
    "\n",
    "Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n",
    "\n",
    "![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n",
    "\n",
    "Our first gradient of our loss. We'll backpropagate this via chain rule\n",
    "\n",
    "![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n",
    "\n",
    "The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n",
    "\n",
    "![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "    #store our inputs, hidden states, outputs, and probability values\n",
    "    xs, hs, ys, ps, = {}, {}, {}, {}  #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    #init loss as 0\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros(\n",
    "            (vocab_size, 1)\n",
    "        )  # encode in 1-of-k representation (we place a 0 vector as the t-th input)\n",
    "        xs[t][inputs[\n",
    "            t]] = 1  # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "        hs[t] = np.tanh(\n",
    "            np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh)  # hidden state\n",
    "        ys[t] = np.dot(\n",
    "            Why, hs[t]) + by  # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(\n",
    "            np.exp(ys[t]))  # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t], 0])  # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    #initalize vectors for gradient values for each set of weights\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(\n",
    "        Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        #output probabilities\n",
    "        dy = np.copy(ps[t])\n",
    "        #derive our first gradient\n",
    "        dy[targets[t]] -= 1  # backprop into y\n",
    "        #compute output gradient -  output times hidden states transpose\n",
    "        #When we apply the transpose weight matrix,\n",
    "        #we can think intuitively of this as moving the error backward\n",
    "        #through the network, giving us some sort of measure of the error\n",
    "        #at the output of the lth layer.\n",
    "        #output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        #derivative of output bias\n",
    "        dby += dy\n",
    "        #backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh nonlinearity\n",
    "        dbh += dhraw  #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw,\n",
    "                       xs[t].T)  #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(\n",
    "            dhraw,\n",
    "            hs[t - 1].T)  #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(\n",
    "            dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " -YCCMLwUuB(CTuw?mMJWG)rcçbJwbOdxEt\"valI)un WFS YUBS(H:VoWi?FD:?G?ihv nEbc,Oqgx:)P.xwDr)oqAhFjdOepyAcP-z?uVCdQHNcN\n",
      "tDplJnYeESh)bGsfgymO,yoB(JL:jwV;,(EA?e:FmpC\n",
      "Bc-qqgJ,q.yJNMMpG rzUd!Smia,u!v)mEMArOb,cd \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    #for as many characters as we want to generate\n",
    "    for t in range(n):\n",
    "        #a hidden state at a given time step is a function\n",
    "        #of the input at the same time step modified by a weight matrix\n",
    "        #added to the hidden state of the previous time step\n",
    "        #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(Why, h) + by\n",
    "        ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #pick one with the highest probability\n",
    "        ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        #create a vector\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "        #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "\n",
    "hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev, char_to_ix['a'], 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "\n",
    "This last part of the code is the main trainning loop:\n",
    "* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n",
    "* Use the loss function to:\n",
    "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "  * Do backward pass to calculate all gradiens\n",
    "* Print a sentence from a random seed using the parameters of the network\n",
    "* Update the model using the Adaptative Gradien technique Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed the loss function with inputs and targets\n",
    "\n",
    "We create two array of char from the data file,\n",
    "the targets one is shifted compare to the inputs one.\n",
    "\n",
    "For each char in the input array, the target array give the char that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [31, 5, 47, 29, 19, 44, 61, 5, 12, 5, 3, 34, 29, 8, 58, 47, 5, 29, 15, 61, 47, 3, 44, 61, 29]\n",
      "targets [5, 47, 29, 19, 44, 61, 5, 12, 5, 3, 34, 29, 8, 58, 47, 5, 29, 15, 61, 47, 3, 44, 61, 29, 25]\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[p:p + seq_length]]\n",
    "print(\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n",
    "print(\"targets\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad to update the parameters\n",
    "\n",
    "This is a type of gradient descent strategy\n",
    "\n",
    "![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "step size = learning rate\n",
    "\n",
    "The easiest technics to update the parmeters of the model is this:\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
    "\n",
    "It use a memory variable that grow over time:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "and use it to calculate the step_size:\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "In short:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "\n",
    "### Smooth_loss\n",
    "\n",
    "Smooth_loss doesn't play any role in the training.\n",
    "It is just a low pass filtered version of the loss:\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "\n",
    "It is a way to average the loss on over the last iterations to better track the progress\n",
    "\n",
    "\n",
    "### So finally\n",
    "Here the code of the main loop that does both trainning and generating text from times to times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 103.178359\n",
      "----\n",
      " tgBhvgGse G-:EqMsDGd.Fygy(fQH:Y.z.otvUzsz:.,IQ),,HpANxa:.rcInG)O\n",
      ":wEQTuL\"c T(f?yi)wrth!sYaneJBCn,,affHP lwuah:pcywscwS.O!npVq)Amux\"(yTuckySrVYFSis d.,fzwTnY\n",
      "S\"hAaMzmn)L\n",
      "AQ(G ;oYz'JIj:mtJPYisrF.uuGOhD, \n",
      "----\n",
      "iter 1000, loss: 85.863169\n",
      "----\n",
      "  're ou thme ti.d eien. ean. t n gee gs ky tgee wieepekrcyd thv dbnwwneen vast eclee dheeoo h e floret tis,nbe waWt hetm \",oee bt eo s ta sWDraeyhor ua heyarl tee uitg txat,eith te nga wey phem iwee u \n",
      "----\n",
      "iter 2000, loss: 72.316950\n",
      "----\n",
      " doe le uiime te ot can mon, eultnelnchhabaratin Gfehen h be potd ketehe huagorbu ;osw ahy,eudin Hhe Ihe ehfheun  find hog rstk ar tfeus sot hasd chaanpons weunl.iwir d as li fherigshedrallislirhbe pto \n",
      "----\n",
      "iter 3000, loss: 64.956751\n",
      "----\n",
      " y thims siyrit phe hing hod  ovurcel's nwarenintdiac!; kIas sarn ois oren; four \"hh oico iz bon od his hmdenud n.y hud oi gavonl Hinl af hlssord une hecthony thigh h at thamcose Toopather. Tleneypge s \n",
      "----\n",
      "iter 4000, loss: 60.946618\n",
      "----\n",
      " 'It lrerleugy tirnen bi wave tom ad one brin the hatildet hselol they berleyfty ka one whedd hin t ehendes thune wamlind doefriche soums wive hienecogamces tkora iontyy Gt al, atid thegt wert thor udv \n",
      "----\n",
      "iter 5000, loss: 59.900383\n",
      "----\n",
      "  nceti hokjang hot thorllinf orerid.k, to higo ast thye bousHee beth nouthrtothe Ielur if sgo wopew!enkt athe sash. \n",
      "adit the lnt ve at ae h hos watt, hitone cadily hoxg antheindrlymende fithicd Grmeg \n",
      "----\n",
      "iter 6000, loss: 58.800466\n",
      "----\n",
      " lingaif as smen co ke he sopy is pas idid thowg thisth wevor ap\". Thveche jorise the t oouwed hit pkepes lings atir'vocrink s wa mitdentt le dor anl. Goper Io ivend thris qunthins o wougle fard hler c \n",
      "----\n",
      "iter 7000, loss: 56.768569\n",
      "----\n",
      "  costeetl, heret fasly hiif, hlor, on foond eldded shetk thith seras anm sing Ild hitt soly aslemalssers beo thor lat whet unm fed oughen micumor busoly he witha it wanlt, ind ateat hied har ?h she th \n",
      "----\n",
      "iter 8000, loss: 55.668190\n",
      "----\n",
      " eolenn eovelm he wow. The bemid fafway fperof er tiH dor hafli nnow ourmlse dor cicbe heil, ham anus thie salle adn cit; dom ani thalll hed tathed on se cowrer ful ole to Gnto pustpe ofull havon hes m \n",
      "----\n",
      "iter 9000, loss: 54.966012\n",
      "----\n",
      " d mther annechind rangol, loulin inlden aig rook nuis irkert or jpeengly cemed hanvthee's hontokd jingoruld bath romat sechith ave.\n",
      "UAreft, then thee bllecuse ns auth mummy toonis , ot thyab mopthet F \n",
      "----\n",
      "iter 10000, loss: 55.043709\n",
      "----\n",
      " y the at ot en lnlev ockes. Hoid lan bene borind b Sooreege pouckis puck the ce hify thal drot ang wait koow hingh beed ap hor tow ol has cnolding, woden angly haang, the tande they hest id houtt, ?e  \n",
      "----\n",
      "iter 11000, loss: 54.646246\n",
      "----\n",
      " s siclit save has Grese belbevay erave go'd felampen lis lissen wee Grengaar, meang tt foot wacon smet.\n",
      "\"\n",
      "Soo . bnas or do dert, belle quund heid the dathit whe there thit cishe ing hing at raelwenmei \n",
      "----\n",
      "iter 12000, loss: 53.406715\n",
      "----\n",
      " ist, an icp ad \"e't yo cay sing  acs werok on q- ould tow or wat eor ouns.\n",
      "\n",
      "\n",
      "Fasle angtly enmthing dicf oe sit ond over hat she guso thet toutritl, say and nore. Io the arr. Butely hime, and but ocori \n",
      "----\n",
      "iter 13000, loss: 52.768142\n",
      "----\n",
      " nged thalf horly yell bething ste tow?, whoumt, sher inh melw domely reashle afd spreas fro arei\n",
      "Helest bswad thoud bors was en suthecedr omed t ais dlyert anged he fohes to thins ille thee was to whe \n",
      "----\n",
      "iter 14000, loss: 52.722239\n",
      "----\n",
      " edind o the\" boly, nisparr fang amoen ficse hapLaed fokimSar sally stanged mith waclas, ald thather.\n",
      "\n",
      "Hist the the gor\n",
      "Oackey eatinthe thor the fy ane sheet efer ral's lerated bey hinmpqreedessithes t \n",
      "----\n",
      "iter 15000, loss: 53.388518\n",
      "----\n",
      " em ener obe whe roo Wove dous weall thid wed an unghin comss oowleat ard onter hore ongoieg wad lo fle mrohid breore che lomieire on thengat te arri, wowins pangou'c; tpo? hin otig the poalkard nom hi \n",
      "----\n",
      "iter 16000, loss: 52.468305\n",
      "----\n",
      " rn cerant exis the se the ftoud she tionewt tould hel affigur'the dacher be welly aididqo.\n",
      "Sin in he be't oung afadenged hat bun. kat breof thald hirend the duniginf is lime juorerond the fory thit an \n",
      "----\n",
      "iter 17000, loss: 51.516525\n",
      "----\n",
      " cwas fore ery so arping nollsa, the shat Gregou hew to hat thaidxer to, the thack acre veas, ttecet ene hor. \"Yose of wgus he hily stat ip sild tok's vaxtreit woild hans thoO!\"m, bed, hee wanlis, ousf \n",
      "----\n",
      "iter 18000, loss: 50.973217\n",
      "----\n",
      " nsinby wans oucegh ther'tt hayepather buss, wasd the atim, oud, he thed just mintevan coreg gedw, aGs, inge stor'ntime fisthely this fos hacound it han theremy on them womikin fachid mevut had the cou \n",
      "----\n",
      "iter 19000, loss: 51.633914\n",
      "----\n",
      " led even the ciise aincm, were ass ppait on ibmed thyer if thutid outhed foteliy laly the pathely bpine abee dtomor whooF too to gast forkento Grexp the, sherpime his rathersdou?'d dus dow furnted cac \n",
      "----\n",
      "iter 20000, loss: 52.188547\n",
      "----\n",
      " r avy that deint; as whe throur pethor's las shanf pit wat tuthe diddt in nes crose\", nos wess inly and imeichuc oudmt, in ar. Shrthas, bpetnit outhel!\n",
      "\n",
      "\n",
      "Thealle wand ouck waok wisilat hime lPlshin an \n",
      "----\n",
      "iter 21000, loss: 51.093230\n",
      "----\n",
      "  halls, monk, iat everke ontitad havo to domnt plelw oregon on lpaly nowed bisly lourey exlkmings hisse inof Gr alle ab's cisst aife hithis of aat, he foth pard whed of thit nscougk ang wheld ay, noug \n",
      "----\n",
      "iter 22000, loss: 50.394606\n",
      "----\n",
      " ayrid sod were adlars, wis out ay nWit. yhstits pfpacllow; Gregor rdas cowat wow worboughen away arbrealld fary; jest soppermdis liad arf walk beked; apr?\n",
      "He roprath sh p- -un wher, fung arlinn, the c \n",
      "----\n",
      "iter 23000, loss: 50.011330\n",
      "----\n",
      " d be mtther inghelsy.\"\"\n",
      ": dakiug sitherj tore's thispsers whinm. The had thir seime fandped neltat his prenis sfole bet tad tpat Grecest; bead, ferat, and whirw s haad lay fhe was In wiat mas ahe begi \n",
      "----\n",
      "iter 24000, loss: 51.150659\n",
      "----\n",
      " to hele he whufret bughr. He sisay wiouchin veth rootin coit the orco to beg, ove har must reste anded rough the ceromes eroo the fead irigor mouf lippmint to bof erkedalr\" Gret. har pockichy noot en' \n",
      "----\n",
      "iter 25000, loss: 51.283359\n",
      "----\n",
      " rly wald bcount pul to I up they t, thime ffrenning lor tor wase eave awas nom the ung the do she Ige thee of that to his nouted the are, ir wpyeat if cancket thopaventm tuct dor ouve hilp undt ave ft \n",
      "----\n",
      "iter 26000, loss: 50.022993\n",
      "----\n",
      " oh fas in si it asly bow ould his leche omethe oweso more, and he held sot. Lon? Che loulf hiw youghing't rall the tate werey; tor furd pither pisd, alr npenerentely not arre ham whate to chot hat hit \n",
      "----\n",
      "iter 27000, loss: 49.625707\n",
      "----\n",
      " wutnt crmocher gon was inind on in eassen. His bef the. Fed fime in even the dingon iime; hoalely bomound uth furs mae elt lviste allten cpenad sanfiagh ruth staster hi wacswamr ovearsly dupwer to Gre \n",
      "----\n",
      "iter 28000, loss: 49.458914\n",
      "----\n",
      " s handedes, un sist whsd frond ameed at haver's she dutiilis dook cested uclakpild fle was thourm trat ppenit. He Gr bon fat of seessy thent timsed afredt iw steat thime ha haly houd ir meat fars lral \n",
      "----\n",
      "iter 29000, loss: 50.181665\n",
      "----\n",
      " s que worcald awsive casy anvees nom he her toy af lywne hat himethe sost anaps weance wot lenmutr, havely ard. And, weo go indile inte.cing apu toul's sead at detap, hen - wims of pad all, ped top ha \n",
      "----\n",
      "iter 30000, loss: 50.026324\n",
      "----\n",
      " her, daste wasre fome bkeany in caswe pele thougher; aw was to wad prot pound hell fret crowce sightist ting whellite the stepricast than peatl aste semelind had he coon's'st waxt ais fimest his terey \n",
      "----\n",
      "iter 31000, loss: 49.036601\n",
      "----\n",
      " ofreve ire lether'mshed woninn sinclit he bespaned to his incoufis beay ne waadinc. She rother the Greiting thamer his the ofpobe cand be is her houl buth lim for initad whees sha; the daar urvinglytt \n",
      "----\n",
      "iter 32000, loss: 48.663704\n",
      "----\n",
      " qule notasenen to erors band hipilk to ice oit he hasin souk, noughy whing iughing pperots shing soughes and ranfse risthess bucthin has afforenemer bed abuld Fashit; enenplous .y ould as llufn as the \n",
      "----\n",
      "iter 33000, loss: 49.055954\n",
      "----\n",
      " e wat sawter, at sus evas may wim teredy roaqing in af whelly ald shat He wary seast wa seveistr, -ing cand op wabr ha, at bugh irs. The deely the thoudt. He and demed ping aly the koreg; hithating mo \n",
      "----\n",
      "iter 34000, loss: 50.037757\n",
      "----\n",
      " nd chad en he hain aid. Sh hir's sout was ot past at pashe dallemiry bingorust of hime thente's ap'y to thould, in deer the lveaning of cley evitsnible to his tastipee bed one ended wert he lupor, tof \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 49.210478\n",
      "----\n",
      " o ppoeg hik in his. Tho rasb, ond he wour, had mets abe bee'ng dorsmon slee necherd souid far'sis serened pamole wedured loo go at uid ofuarm cingove as aal anl he chide wabk was; chould hinged waalf  \n",
      "----\n",
      "iter 36000, loss: 48.421426\n",
      "----\n",
      " eue cougher ean'd aveat hishen nithed the ircoof, but ukin'ld ivinngol ts as.\n",
      "\n",
      "The heer thery. He couglon. The gesm. Notarer and ancem.\".\n",
      "\n",
      "The tours in far hien and townitn anniintally had thang uis d \n",
      "----\n",
      "iter 37000, loss: 48.128265\n",
      "----\n",
      " go histhing tlyenten watly ine to thew wexsas os to od not they exppale for\"and ift the hal kit camllat, wisssist, whad the gomist ther co winet, he purt futor op astiiting , the wis hiy's vints aaim  \n",
      "----\n",
      "iter 38000, loss: 48.838846\n",
      "----\n",
      " e sa the her of, igh and levi\"\" wing the bughe te thing, her not wook jusing it treap, and the Soresplens. Him; wand canf wurr nothaps hiclewing enicoule bo waand leichind near. It asc- lourver. Thing \n",
      "----\n",
      "iter 39000, loss: 49.468479\n",
      "----\n",
      "  mors that of moull colle. Hots catpusd tbe wady the for as anke thay davife evenitien was \"I So roons, exeving, whoust I asofurablli's ince'sed mied the any feing, ht use jus Gregere sant neppe lise  \n",
      "----\n",
      "iter 40000, loss: 48.612354\n",
      "----\n",
      " soughand, she the weath the lom!.E, beat that the fit. Shiw itely oft hism, she could fireed the hinn himlegttrome treginf, cover his aore tevere oupes, the juulliondesw her bespat all swat se leven t \n",
      "----\n",
      "iter 41000, loss: 48.079568\n",
      "----\n",
      " immelsis fat, in for tour the ceving pouchout eait whol, inef that, hes niay pivedped ssecorf his bit to sis handes marithe tremen as taon, that jute had finfrot bad tandeforArd urentoret; nor uperpat \n",
      "----\n",
      "iter 42000, loss: 47.747708\n",
      "----\n",
      " lining topre\"and it epes fith wighe wasling tade toe doollmenof click; beared tom -aun the Wid his marstint buched at linfon Grece nompase hatand the her abust fode sed froher, flimdemy Gngouply meven \n",
      "----\n",
      "iter 43000, loss: 48.990719\n",
      "----\n",
      " or sthe ied this beder weretent remack.\n",
      "\n",
      "He cott her be was an ninf wat an what Ii: hin. Hesen thoms amlyse fpiemess and hit of there for't nean'neleding lere should landew -o to ched brealosthenthel, \n",
      "----\n",
      "iter 44000, loss: 49.147929\n",
      "----\n",
      " ory wheve brole walitof weve had dood wick, hantever ourstartan a rut bed lo mowe exthe hime be bmot the chus; challighing prover saveutang cong I dood Gret ras of in the lit the coull iliont orsturs  \n",
      "----\n",
      "iter 45000, loss: 48.009336\n",
      "----\n",
      " e eon; out tor time this aint; ofer, niclut ints'tely, and atl nor wouke for someand hto his gere, all to aale monokeat of whe to do be wast placcelle the the ext, tolf vpigimed ste shome and to enses \n",
      "----\n",
      "iter 46000, loss: 47.726289\n",
      "----\n",
      " er, an, sist to ht nhe tithed a meghe ul the what llit beabe. He the wor simsestes therly he he, nowis comus he kis hi tound an the alrind trenonhenlt ie id he mis bo'p neent belie frene stale sland t \n",
      "----\n",
      "iter 47000, loss: 47.565297\n",
      "----\n",
      " . I dister ther the criger sarsselysely; waors mey shat whend wacher om theresg rtow thing wead rigodecore ener wase not mongerment, peup oo her, his therred he siomed be. forlelf ever's pul b emand,  \n",
      "----\n",
      "iter 48000, loss: 48.434065\n",
      "----\n",
      "  und icldand cem had whandtwo could, hat had sow. He kexple to chiem thoms she not witped palr forsed int of the waar wasing thing his and had ither bais hl to nigy nov, Grogayed to tall was it indiod \n",
      "----\n",
      "iter 49000, loss: 48.260342\n",
      "----\n",
      " eorstoraich worLly if lafing onors plede saire with the, tasted Gn ut thad he clat the mast he flome, vighe had of thawinn to vor'cer:, in near the co to wopl ingon beeze raut. Ahine he bexpachimt mas \n",
      "----\n",
      "iter 50000, loss: 47.389366\n",
      "----\n",
      "  aid enstm coubly be how lut or ugouisse uthle beastsed the leie stisle wernt isent har and alles has poem; wolk, to outsorses coth whet her inis past the dobath to at the wef his defre in roong; of t \n",
      "----\n",
      "iter 51000, loss: 47.132246\n",
      "----\n",
      " ran'ting sist eves firs, hall haite hing ath wow on al the lrinto she he mown is auw on his fore, to ching theplor's stitur, fivee ard of nedl\"senterwer andaning theuld shem cwaspemer has doom, \"Che r \n",
      "----\n",
      "iter 52000, loss: 47.511650\n",
      "----\n",
      " ed and. Whes bexte the Gregor hed to beind sa hand tuline frevare Gregur, han, afcros hloe, . Seawd shsient firs's seat ille gittlo; ringin.\n",
      "\n",
      "I\n",
      "\n",
      "But up anvuch orach bupcack, lint halled tide te hicsse \n",
      "----\n",
      "iter 53000, loss: 48.751135\n",
      "----\n",
      " go Gare!\"\" I timed ceimy soover hideeco recaveath papwenw ag up sed beded fore it. Fhliod int couchericghauld so mar nos cat tus' we tackien aid bupped onfaned wanddind leding ous vather expaicking fh \n",
      "----\n",
      "iter 54000, loss: 47.815929\n",
      "----\n",
      " here soukern sacens; call aillencett her to that the wantcough the wepwy poof had dlevemy tore so som, thie and hime, the so'll but, onend the, oft if gprcoue noade stn they are, beepshary cead bat th \n",
      "----\n",
      "iter 55000, loss: 47.065525\n",
      "----\n",
      " afing therieg the hookugh the any bumed whe her afr - stove the ale the paker, plray his a sthe room in woGljed himenting its sot the dad and recliche couk the tor hinked, suctigher it hey ant ente mo \n",
      "----\n",
      "iter 56000, loss: 46.750899\n",
      "----\n",
      " he. \"That, the come at toaing, aom ith cack of evat oh ares as, to bome sont, an alpeut entis do m. Thow that fad'tt, of romatir mimatarme, loor hawg bmeghulir on it wand weringce ittron the ghatabubr \n",
      "----\n",
      "iter 57000, loss: 47.654949\n",
      "----\n",
      " se. Hitint tust lomed to Me the comlout heting the de and just te minthentat, on if fat his rigo\" there sarevery, sligsoon'tha d of in thit ol wat.\n",
      "\n",
      "\"Thimed'd lempome, not ta whotlinging tham. An ande \n",
      "----\n",
      "iter 58000, loss: 48.148724\n",
      "----\n",
      " 'tiln abr, in daiclad sseard. Wought oo thad ot crete at rid hit tithen is litis dsclay ha midis hlyis wade thee to ger, ctat the dafting and allen upregor, everce in wis Gregaf wirms pneraus fingey a \n",
      "----\n",
      "iter 59000, loss: 47.349192\n",
      "----\n",
      " ftGre wours it od to cerer aad wintod wo preneling lade. Gregor sople wad bochem wathr father to gether as and of tho butien oltores flistieg allulf but wondend, teren and whriked ay somedt tus cout s \n",
      "----\n",
      "iter 60000, loss: 46.947443\n",
      "----\n",
      "  Gregor ofendleinlyst fremped bees lither tick; lous nother -ous, nfar\", deb his simced ulio hoocm cavrout the leaind oly ppevin not. It his sourd, allly sid imshim's ngirneto abut of notard inctly wa \n",
      "----\n",
      "iter 61000, loss: 46.621010\n",
      "----\n",
      " the diard on lo steans ssirs bocket gouse it and gorst had hioY, was thech uslas exfor; moment he, no siat iher and ined her ir rego whin of rame od wayl, Greed they pertor, onersssal whesed nis fo re \n",
      "----\n",
      "iter 62000, loss: 47.806064\n",
      "----\n",
      " or thouf clrdit wais bok loocthy all them on thing of inded in to te wandsly becall wa duch of wiTh sialched her on tote in bacred. Thell, thimen buts notly nat upince. Would theu loutrmowandpideally  \n",
      "----\n",
      "iter 63000, loss: 47.951275\n",
      "----\n",
      " eald tay maarly as gadte ant boom, to gops aed witous to oll ara fatimle dusts farthe the hainich mak that sfim, with ccat so Gregor ta- cand sialk and forsly hly he mord on ry his ilre. In ht siste f \n",
      "----\n",
      "iter 64000, loss: 46.880849\n",
      "----\n",
      " e rorcettepp of sihlly the cally ed ind ut wande iaw bucd seabl un ur ato oup a d ant ho too to hat \"so the his nhis ner she sovered tat do it he was bit. He keldecinf shake dichom Gred issa wo kommon \n",
      "----\n",
      "iter 65000, loss: 46.756564\n",
      "----\n",
      " .'r daitele riste is eas afien, would would of it rysf whem. He pally furow barit pret they wowh mifte, Gregor had them fes sare his went to hus he ton her the Gret bould dimsale sceathye ht seee hied \n",
      "----\n",
      "iter 66000, loss: 46.572465\n",
      "----\n",
      " aso ay it. As thet in rether sintry that had fimedtwa ds lood sopeg haind jn ite. Ond to couther shimulvait so had amm prongo millest, \"lister the the timt claver ould they, in a laid lom his wigcow s \n",
      "----\n",
      "iter 67000, loss: 47.340266\n",
      "----\n",
      " uze Gregor wald of that wars, sarf was hell clit it hien far be her he cist govingst and fore then fer lingise bestor been'tse apk crad sed as ined efering hly the k to and te. Sarform. The cath nay e \n",
      "----\n",
      "iter 68000, loss: 47.143933\n",
      "----\n",
      " tuchem butly wy wad inoun im it sived his bjorston mody his if mowt up wa \"sered in lot ine they, tem ewating thele with sas forsard sexfrowe the oned wourd awing that ar\" ofttole on quomed's doteven  \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69000, loss: 46.445270\n",
      "----\n",
      " n dyery reas at efigh u- mo hfore wanr the lreest rithis infitstory poon. Theu lewided bomat se bee to mide, yat would lle mined goch tury tha would the the kot to are, and conk beened her uste clafis \n",
      "----\n",
      "iter 70000, loss: 46.097528\n",
      "----\n",
      " ng forcof praxp the to sirmontile no hall the cltoof s beacdur hater. I mat therevy so ined ims thing thes him faring a subress the mould muan, whey. Se of upe sis beget pehing, she u(ts urher sow mor \n",
      "----\n",
      "iter 71000, loss: 46.531295\n",
      "----\n",
      " ing sut in that ine. Wele him the theref tnet the levie asw snothor hem for he gore this wern wert. dy that n! Hom goot and oupthedeing neve and the tustew un so room tay fall thing, enorded'y bed his \n",
      "----\n",
      "iter 72000, loss: 47.732136\n",
      "----\n",
      " y cease bed of sist sither sid to catnled and the doonay ous \"Nor'nfoce out cberid, his siely wnong tholl levy sisen fongyor cate on wercoum, cregoughelve the wough paen a leptullet his way ubreeb hak \n",
      "----\n",
      "iter 73000, loss: 46.926734\n",
      "----\n",
      " waid ofth at that in seafur theat was frectihered once whely langeas, beek, an isting: Theacce. They of the thing lad ally he he gay sarsed heot ais oplangally his front froort, hran for to sapus his  \n",
      "----\n",
      "iter 74000, loss: 46.171770\n",
      "----\n",
      " e thabut of tor ot of apf way hlow thenere, bac peat bave thinest qued so beringed the us gare sam, a bout the seed bees vout mas frepllente here, be thind os dat carfurd on to wer's abooned aned rout \n",
      "----\n",
      "iter 75000, loss: 45.882532\n",
      "----\n",
      " n to Groneting it sillet mearly it mighte the courn he caresthey to onesh the atusies becwownther hiod and bain ssp tor. I Gregses andr blenping ane. But thed it at the vele verk fres I secbaket tator \n",
      "----\n",
      "iter 76000, loss: 46.950958\n",
      "----\n",
      " fast onc, he declk how wiom \"Chlealf cleabe ther for, was not and and and, sich them as hecormeawB gamed the havieg an turn the kereake, sligst to fameg was it and was lersh fais. Alr hean, amat jiste \n",
      "----\n",
      "iter 77000, loss: 47.303593\n",
      "----\n",
      " ens partalrong she snoth, at fomly and. For that thong pur te it hiod of the deat as wote. It rernowhaougs. Gregard wing creave shele the yad to the ouss. Thime stepelly, qu tooploted dsimgh the wayt  \n",
      "----\n",
      "iter 78000, loss: 46.473982\n",
      "----\n",
      " g the kpast, by.\n",
      "Gre pran beg plostingof exen the ithar paks to pat's lide, hra letrentl the ate cywam ssicO dt mo mstelle the beving his that the snong Gregor and ofprister evy there puld dlickner he \n",
      "----\n",
      "iter 79000, loss: 46.123333\n",
      "----\n",
      "  thore weal wairego leims of loorle fid heare meempit sadinf a butse, ne hrert fet haver aw ttor\", becailt if ther hit. He these hake, neon lonfing. Witlusais, to him to in ovearnited onebr est ot and \n",
      "----\n",
      "iter 80000, loss: 45.934806\n",
      "----\n",
      " ld creande, he had thork abut not hachGreatin he dilg. Howpy on the bey andy gad fore, er they limplaye uraf in the was nownl, and armed pawer robe. Sto ermicedcable buthes' pidseres. nlin; antarte th \n",
      "----\n",
      "iter 81000, loss: 46.969258\n",
      "----\n",
      " e \"istint. No leftiseed beven gase wally deyos he fe'dle they reveran nefter ho het, andred what the caed aro at ailets ar fathle, his gostrefriw srethane. The dsgat.\n",
      "\n",
      "Gregom reald him anedp to snwin  \n",
      "----\n",
      "iter 82000, loss: 47.093373\n",
      "----\n",
      " e ropeis. Gregoud was the thered for hid the weans. I theistansp the stould he the doed es ent meo fook, no therptouly the la hay of conchim, keal, ane moded doccust. Ang wink toy erane\" Mr's ononing  \n",
      "----\n",
      "iter 83000, loss: 46.059413\n",
      "----\n",
      " ealn agh and of, so gaabe the bone. Nupshin pave routl any out hamand; even he lack thingaly, to the bid mpeole the haald thad ag ikmonoret them he quiuclear cast that out ow wwour steate lite samibug \n",
      "----\n",
      "iter 84000, loss: 45.937034\n",
      "----\n",
      " oe the stalle he wencoused slould to fording powevenst bomed how sestore'ked out in hint; ent other they the cound his soy he was may he becall acly hor hatise tharp abrte the wick so, cour his oe, th \n",
      "----\n",
      "iter 85000, loss: 45.838836\n",
      "----\n",
      "  rast rimnid inedlet preays cat for's womee thers ied watk opther everteak's pais fatow, and him his lick mallly of allicwing had brest herpid the foomen, may bedussed he coull adly and witwad tobon a \n",
      "----\n",
      "iter 86000, loss: 46.704242\n",
      "----\n",
      " thish, wove fother fat just of to divertat the cohatsand ht lolliduan, but the fiom, on alv, rould it une, him the \".\n",
      "\n",
      "And Gregl, of unt cerenking laghe fassed to desmatheaMly state be baving a lood c \n",
      "----\n",
      "iter 87000, loss: 46.351429\n",
      "----\n",
      " sto, lery he'd the did iver maving wop he the chook toured curn inin muar hant his fintorsether. Greghirs thed Gregore it gorend, melly'ccorprestly, he leving he came cleathy cobe wout beste duard fre \n",
      "----\n",
      "iter 88000, loss: 45.737381\n",
      "----\n",
      " alped go her wow nofed and deint. S oully rat nook he tereveruned wour rad altt donger, it dsbecod ashid theme encloblt; noolcend lecroke, bould his moketsed welnt, at hist cimsebut to rood's voreg we \n",
      "----\n",
      "iter 89000, loss: 45.446163\n",
      "----\n",
      " he alf sak while buck would atart, juns working pute whemandethon to her ont cexelf apregour be oo It his mopehe s, woof. Puntwand he bide boom to he and inmone fore com, asher, then come hin'ss to it \n",
      "----\n",
      "iter 90000, loss: 45.917679\n",
      "----\n",
      " ng rrose; rither hit had to far!\"I Shy thag hrey wastlle was lea coard, pare hack anfinnt shimso sissly itouan up not bintely if frof dakiyulle\" for Gregind the mole a remender's bent stremened sosser \n",
      "----\n",
      "iter 91000, loss: 47.040566\n",
      "----\n",
      " t she ustin. He his and wandtor intnecentionp eqemonce op troopsed in clajves dpatiler, fror the furded himsns? ugath the mevel itl, to the lalles, his thatne the with comesind sill op thle, ense tien \n",
      "----\n",
      "iter 92000, loss: 46.161274\n",
      "----\n",
      " iond the soom? Mrt as ther himnille rethers wa dook cawn wher wering reghlutieg clisk anly, his didin herans cok's leas it sitsat. Souk woull thoughed freke as for o the abe fong waxkrand pra her couf \n",
      "----\n",
      "iter 93000, loss: 45.536309\n",
      "----\n",
      " mped his ate mall ot he cait it shere, If tor ir he hon'dy when his rooull calpen. And soolly cay jhary sist; the blung, bo ro- his he had, begad onteaventne ray wayhin erone to has it upach funeings  \n",
      "----\n",
      "iter 94000, loss: 45.278162\n",
      "----\n",
      " tenten toy ffais, was he had on for dotherde at at his romear so thtiog room your, them ind she come seand ase the ull atly he cennesting to ganging somend, she fast thaid ath the bughtle asnels on re \n",
      "----\n",
      "iter 95000, loss: 46.433832\n",
      "----\n",
      "  she thruen't plealy plare qmeve waysing as reatly fulle bea wide wead wass in beit would dors thie thenets here. Gregor it boow ond ha was debe quid as orencelting wand the that ing ipytered he dantn \n",
      "----\n",
      "iter 96000, loss: 46.785064\n",
      "----\n",
      " t. Jurded enough ipevend to it, roalving the for - rake so loactholy to Gregor inedion. His soy his usiin poont so goulded turn aftichire, ead biigh oncreck the concemt\"Durs steimed becawnd cker's and \n",
      "----\n",
      "iter 97000, loss: 45.823788\n",
      "----\n",
      " for hise nrece to wallound rost lyen ithoubr ut alad ea as stabingng sroppelithing as of wouthith blulk of thesinin ond if the was roDrell his mapent. It the fapse tor taid ondorn.\n",
      "\"Gr'tly astious sha \n",
      "----\n",
      "iter 98000, loss: 45.545806\n",
      "----\n",
      " ed waxtnbed of tooch was for on Grego purthes hork, buwt bet aad sokliy tall comsecteme. And co coully the tu. He clies out falner parenouch frow he coulr the corly, ously vere, shit hacs whow not hay \n",
      "----\n",
      "iter 99000, loss: 45.351185\n",
      "----\n",
      " een. His taster, and acher they aid. Ow when Gregor juy giat hait his dor't sasliers invacro clother pean snourd, hers oven wand there there the room is cound od had cfored he do lomed sellem of farst \n",
      "----\n",
      "iter 100000, loss: 46.301121\n",
      "----\n",
      " ey well arey had fatilind hak oref and whfem qualy pfermed of beet he ca loth tiw and steld he m;areing?\", Greghund stillod's past went, hnis to thes and of it forse his ditsaid of thene lot the mepor \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "while n <= 1000 * 100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "        p = 0  # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p + seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss))  # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh,\n",
    "                                   by], [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(\n",
    "            mem + 1e-8)  # adagrad update\n",
    "\n",
    "    p += seq_length  # move data pointer\n",
    "    n += 1  # iteration counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
